# Professional QA System Implementation Plan

**Created**: 2025-12-01
**Vision**: Transform ORIZON from a basic artifact generator into a comprehensive, professional QA platform
**Reference Tools**: Jira/Xray, TestRail (industry standards)

---

## ğŸ¯ Executive Summary

**Current State**: ORIZON is a functional but basic AI-powered artifact generator with authentication and history.

**Target State**: A professional QA management system that combines AI-powered artifact generation with test management, execution tracking, and quality metrics.

**Key Inspiration**:
- **TestRail**: Test case management, test runs, results tracking
- **Xray (Jira)**: Test planning, execution, traceability to requirements
- **AI Enhancement**: Use Claude to generate artifacts, but add professional QA workflows

---

## ğŸ“Š Gap Analysis: Current vs. Professional QA System

### âœ… What We Have (Foundation)
- User authentication & authorization
- AI-powered artifact generation (user stories, tests, acceptance criteria)
- Basic history tracking
- Multiple input methods (paste, GitHub, upload)
- Settings management

### âŒ What's Missing (Critical QA Features)

#### 1. Test Management
- âŒ Test case organization (folders, suites, tags)
- âŒ Test case versioning
- âŒ Test case reusability
- âŒ Bulk operations (import, export, clone)

#### 2. Test Execution
- âŒ Test runs/cycles
- âŒ Test execution tracking (passed, failed, blocked, skipped)
- âŒ Defect linking
- âŒ Evidence attachment (screenshots, logs)

#### 3. Requirements Traceability
- âŒ Link tests to requirements/user stories
- âŒ Coverage metrics (which requirements have tests)
- âŒ Impact analysis (what tests fail when requirements change)

#### 4. Reporting & Metrics
- âŒ Test execution reports
- âŒ Coverage dashboards
- âŒ Trend analysis
- âŒ Quality metrics (pass rate, defect density)

#### 5. Collaboration
- âŒ Comments on test cases
- âŒ Test review workflow
- âŒ Team assignments
- âŒ Notifications

---

## ğŸ—ï¸ Architecture: Professional QA System

### New Database Schema (6 additional tables)

```sql
-- Test Cases (managed tests, not just generated artifacts)
CREATE TABLE test_cases (
  id SERIAL PRIMARY KEY,
  project_id INTEGER REFERENCES projects(id),
  title VARCHAR(500) NOT NULL,
  description TEXT,
  preconditions TEXT,
  steps JSONB,  -- [{step: string, expected: string, data: string}]
  expected_result TEXT,
  priority VARCHAR(20),  -- Critical, High, Medium, Low
  type VARCHAR(50),  -- Functional, Integration, Performance, Security, UI/UX
  status VARCHAR(20),  -- Draft, Ready, Deprecated
  automated BOOLEAN DEFAULT FALSE,
  tags JSONB,  -- ['login', 'auth', 'smoke']
  folder_path VARCHAR(500),  -- /Regression/Login/
  created_by INTEGER REFERENCES users(id),
  updated_by INTEGER REFERENCES users(id),
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  version INTEGER DEFAULT 1
);

-- Projects (organize tests by project/product)
CREATE TABLE projects (
  id SERIAL PRIMARY KEY,
  name VARCHAR(200) NOT NULL,
  description TEXT,
  prefix VARCHAR(10),  -- TC prefix: "PROJ-TC-001"
  owner_id INTEGER REFERENCES users(id),
  created_at TIMESTAMP DEFAULT NOW()
);

-- Test Runs (execution cycles)
CREATE TABLE test_runs (
  id SERIAL PRIMARY KEY,
  project_id INTEGER REFERENCES projects(id),
  name VARCHAR(200) NOT NULL,
  description TEXT,
  status VARCHAR(20),  -- Planned, In Progress, Completed
  start_date TIMESTAMP,
  end_date TIMESTAMP,
  environment VARCHAR(100),  -- Dev, Staging, Production
  build_version VARCHAR(100),
  assigned_to INTEGER REFERENCES users(id),
  created_by INTEGER REFERENCES users(id),
  created_at TIMESTAMP DEFAULT NOW()
);

-- Test Results (execution records)
CREATE TABLE test_results (
  id SERIAL PRIMARY KEY,
  test_run_id INTEGER REFERENCES test_runs(id),
  test_case_id INTEGER REFERENCES test_cases(id),
  status VARCHAR(20),  -- Passed, Failed, Blocked, Skipped, Retest
  executed_by INTEGER REFERENCES users(id),
  execution_time INTEGER,  -- seconds
  comment TEXT,
  attachments JSONB,  -- [{name, url, type}]
  defect_id VARCHAR(100),  -- Link to external defect tracker
  executed_at TIMESTAMP DEFAULT NOW()
);

-- Requirements (user stories, features)
CREATE TABLE requirements (
  id SERIAL PRIMARY KEY,
  project_id INTEGER REFERENCES projects(id),
  title VARCHAR(500) NOT NULL,
  description TEXT,
  type VARCHAR(50),  -- Epic, Story, Feature, Bug Fix
  priority VARCHAR(20),
  status VARCHAR(20),  -- Backlog, In Progress, Done
  external_id VARCHAR(100),  -- Jira ID, GitHub Issue #
  created_at TIMESTAMP DEFAULT NOW()
);

-- Test Coverage (link tests to requirements)
CREATE TABLE test_coverage (
  id SERIAL PRIMARY KEY,
  test_case_id INTEGER REFERENCES test_cases(id),
  requirement_id INTEGER REFERENCES requirements(id),
  created_at TIMESTAMP DEFAULT NOW(),
  UNIQUE(test_case_id, requirement_id)
);

-- Add indexes for performance
CREATE INDEX idx_test_cases_project ON test_cases(project_id);
CREATE INDEX idx_test_cases_status ON test_cases(status);
CREATE INDEX idx_test_cases_folder ON test_cases(folder_path);
CREATE INDEX idx_test_runs_project ON test_runs(project_id);
CREATE INDEX idx_test_results_run ON test_results(test_run_id);
CREATE INDEX idx_test_results_case ON test_results(test_case_id);
CREATE INDEX idx_coverage_test ON test_coverage(test_case_id);
CREATE INDEX idx_coverage_req ON test_coverage(requirement_id);
```

---

## ğŸš€ Implementation Phases

### Phase 1: Test Management Foundation (8-10 hours)

**Goal**: Add proper test case management (not just AI-generated artifacts)

#### 1.1 Projects & Organization (2 hours)
- Create projects table and API
- Project creation UI (`/projects/new`)
- Project list page (`/projects`)
- Project dashboard (`/projects/[id]`)

**Files to Create**:
- `app/projects/page.js` - Project list
- `app/projects/new/page.js` - Create project
- `app/projects/[id]/page.js` - Project dashboard
- `app/api/projects/route.js` - CRUD operations
- `lib/db-projects.js` - Database functions

#### 1.2 Test Case Management (4 hours)
- Create test_cases table
- Test case CRUD operations
- Test case list with filtering/search
- Test case detail view with edit
- Folder/tag organization

**Pages to Create**:
- `/projects/[id]/tests` - Test case list
- `/projects/[id]/tests/new` - Create test
- `/projects/[id]/tests/[testId]` - View/edit test
- `/projects/[id]/tests/[testId]/history` - Version history

**Components to Create**:
- `TestCaseForm.jsx` - Create/edit test cases
- `TestCaseList.jsx` - List with filters
- `TestCaseCard.jsx` - Card view
- `FolderTree.jsx` - Hierarchical folder navigation
- `TagManager.jsx` - Tag CRUD

#### 1.3 AI Integration with Test Management (2 hours)
- **New workflow**: Generate â†’ Review â†’ Save as Test Cases
- Import AI-generated artifacts into test case library
- Bulk import from analysis history
- Map generated tests to structured test cases

**Enhancement to Dashboard**:
- After AI generation, show "Save to Project" button
- Select project + folder
- Bulk import with review

---

### Phase 2: Test Execution & Tracking (6-8 hours)

**Goal**: Enable teams to execute tests and track results

#### 2.1 Test Runs (3 hours)
- Create test_runs table
- Create test run from selected tests
- Assign tests to team members
- Track run status

**Pages to Create**:
- `/projects/[id]/runs` - List test runs
- `/projects/[id]/runs/new` - Create run (select tests)
- `/projects/[id]/runs/[runId]` - Run dashboard
- `/projects/[id]/runs/[runId]/execute` - Execute tests

#### 2.2 Test Execution Interface (3 hours)
- Step-by-step execution UI
- Mark pass/fail with evidence
- Add comments and attachments
- Link defects
- Real-time progress tracking

**Components to Create**:
- `TestExecutionView.jsx` - Main execution interface
- `TestStepExecutor.jsx` - Step-by-step UI
- `ResultStatusPicker.jsx` - Pass/Fail/Blocked/Skip
- `EvidenceUpload.jsx` - Screenshot/log upload
- `DefectLinker.jsx` - Link to Jira/GitHub issues

#### 2.3 Results & Reporting (2 hours)
- Test results dashboard
- Execution history per test case
- Run summary reports
- Export results (PDF, CSV, JSON)

**Pages to Create**:
- `/projects/[id]/runs/[runId]/results` - Results summary
- `/projects/[id]/reports` - Historical reports

---

### Phase 3: Requirements & Traceability (4-6 hours)

**Goal**: Link tests to requirements for coverage tracking

#### 3.1 Requirements Management (2 hours)
- Create requirements table
- Import requirements from Jira/GitHub
- Manual requirement creation
- Requirement list and detail pages

**Pages to Create**:
- `/projects/[id]/requirements` - List requirements
- `/projects/[id]/requirements/new` - Create requirement
- `/projects/[id]/requirements/[reqId]` - Requirement detail

#### 3.2 Test Coverage Tracking (2 hours)
- Link tests to requirements (many-to-many)
- Coverage matrix view
- Identify untested requirements
- Impact analysis (which tests cover a requirement)

**Pages to Create**:
- `/projects/[id]/coverage` - Coverage matrix
- `/projects/[id]/requirements/[reqId]/tests` - Tests for requirement

#### 3.3 Coverage Metrics (2 hours)
- Calculate coverage percentage
- Dashboard widgets for coverage
- Trend analysis over time
- Alerts for low coverage

---

### Phase 4: Advanced QA Features (8-10 hours)

**Goal**: Professional-grade features for enterprise teams

#### 4.1 Test Review Workflow (2 hours)
- Test case review status (Draft â†’ Review â†’ Approved)
- Assign reviewers
- Review comments
- Approval workflow

#### 4.2 Collaboration Features (2 hours)
- Comments on tests/runs/requirements
- @mentions and notifications
- Activity feed
- Email notifications (test assignment, failures)

#### 4.3 Advanced Reporting (3 hours)
- Custom report builder
- Dashboards with charts (Chart.js/Recharts)
- Trend analysis (pass rate over time)
- Quality metrics:
  - Defect density (defects per test)
  - Test effectiveness (defects found vs. missed)
  - Automation coverage
  - Execution velocity

**Metrics to Track**:
- Pass/Fail rate per run
- Execution time trends
- Flaky test detection (inconsistent results)
- Coverage by priority/type
- Defect distribution by severity

#### 4.4 Integrations (3 hours)
- **Jira Integration**:
  - Import stories/bugs as requirements
  - Create defects from test failures
  - Sync test results to Jira

- **GitHub Integration**:
  - Import issues as requirements
  - Link test failures to GitHub issues
  - CI/CD webhook for automated run creation

- **Slack Integration**:
  - Notifications for test failures
  - Daily summary reports
  - Run completion alerts

---

### Phase 5: AI Enhancements (6-8 hours)

**Goal**: Leverage AI beyond basic generation

#### 5.1 Smart Test Generation (2 hours)
- Generate tests from requirements (not just code)
- Suggest missing test scenarios
- Auto-tag tests based on content
- Detect duplicate tests

#### 5.2 Test Maintenance Assistant (2 hours)
- Detect outdated tests (requirements changed)
- Suggest test updates based on code changes
- Flag potentially flaky tests
- Recommend consolidation of similar tests

#### 5.3 Defect Prediction (2 hours)
- Analyze test results to predict failure-prone areas
- Risk-based test prioritization
- Suggest additional tests for high-risk features

#### 5.4 Natural Language Test Creation (2 hours)
- "Create a test for user login with 2FA"
- AI converts natural language to structured test case
- Auto-populate steps, expected results

---

## ğŸ“± Updated Page Structure

### Core Navigation (New Sidebar)

```
Dashboard (Overview)
â”œâ”€â”€ My Test Runs (assigned to me)
â”œâ”€â”€ Recent Activity
â””â”€â”€ Quick Stats

Projects
â”œâ”€â”€ All Projects
â”œâ”€â”€ Create Project
â””â”€â”€ Archived

[Active Project]
â”œâ”€â”€ Test Cases
â”‚   â”œâ”€â”€ All Tests
â”‚   â”œâ”€â”€ By Folder
â”‚   â”œâ”€â”€ By Tag
â”‚   â””â”€â”€ Create Test
â”œâ”€â”€ Test Runs
â”‚   â”œâ”€â”€ Active Runs
â”‚   â”œâ”€â”€ Create Run
â”‚   â””â”€â”€ History
â”œâ”€â”€ Requirements
â”‚   â”œâ”€â”€ All Requirements
â”‚   â”œâ”€â”€ Import
â”‚   â””â”€â”€ Create Requirement
â”œâ”€â”€ Coverage
â”‚   â”œâ”€â”€ Coverage Matrix
â”‚   â”œâ”€â”€ Untested Requirements
â”‚   â””â”€â”€ Coverage Trends
â”œâ”€â”€ Reports
â”‚   â”œâ”€â”€ Execution Reports
â”‚   â”œâ”€â”€ Coverage Reports
â”‚   â””â”€â”€ Quality Metrics
â””â”€â”€ Settings
    â”œâ”€â”€ Team Members
    â”œâ”€â”€ Integrations
    â””â”€â”€ Preferences

AI Assistant (New!)
â”œâ”€â”€ Generate from Code
â”œâ”€â”€ Generate from Requirements
â”œâ”€â”€ Suggest Missing Tests
â””â”€â”€ Analysis History

Settings
â”œâ”€â”€ Profile
â”œâ”€â”€ API Keys
â””â”€â”€ Notifications
```

---

## ğŸ¨ UI/UX Improvements

### 1. Test Case Editor (Rich Form)
```jsx
<TestCaseForm>
  <TextField label="Title" required />
  <RichTextEditor label="Description" />
  <TextField label="Preconditions" multiline />

  <StepsEditor>
    {/* Repeatable step blocks */}
    <StepBlock>
      <TextField label="Step" />
      <TextField label="Expected Result" />
      <TextField label="Test Data" />
      <Button>Add Step</Button>
    </StepBlock>
  </StepsEditor>

  <Select label="Priority" options={['Critical', 'High', 'Medium', 'Low']} />
  <Select label="Type" options={['Functional', 'Integration', 'UI/UX', 'Performance']} />
  <TagInput label="Tags" />
  <FolderPicker label="Folder" />
  <Checkbox label="Automated" />
</TestCaseForm>
```

### 2. Test Execution Interface
```jsx
<ExecutionView>
  <TestHeader>
    <h2>{test.title}</h2>
    <StatusButtons>
      <Button variant="success">Pass</Button>
      <Button variant="danger">Fail</Button>
      <Button variant="warning">Blocked</Button>
      <Button variant="secondary">Skip</Button>
    </StatusButtons>
  </TestHeader>

  <StepsExecution>
    {test.steps.map(step => (
      <StepCard>
        <h4>Step {i + 1}</h4>
        <p>{step.action}</p>
        <p><strong>Expected:</strong> {step.expected}</p>
        <StatusToggle />
        <CommentBox placeholder="Add note..." />
      </StepCard>
    ))}
  </StepsExecution>

  <EvidenceSection>
    <FileUpload label="Attach Screenshot/Log" />
    <DefectInput label="Link Defect ID" />
  </EvidenceSection>

  <ActionBar>
    <Button>Previous Test</Button>
    <Button variant="primary">Submit & Next</Button>
  </ActionBar>
</ExecutionView>
```

### 3. Coverage Matrix
```jsx
<CoverageMatrix>
  <Table>
    <thead>
      <tr>
        <th>Requirement</th>
        <th>Priority</th>
        <th>Test Cases</th>
        <th>Coverage</th>
        <th>Status</th>
      </tr>
    </thead>
    <tbody>
      {requirements.map(req => (
        <tr>
          <td>{req.title}</td>
          <td><Badge>{req.priority}</Badge></td>
          <td>{req.testCount}</td>
          <td>
            <ProgressBar value={req.coverage} />
            {req.coverage}%
          </td>
          <td>
            <StatusBadge status={req.status} />
          </td>
        </tr>
      ))}
    </tbody>
  </Table>
</CoverageMatrix>
```

---

## ğŸ“Š Key Metrics to Display

### Project Dashboard
- **Test Execution Rate**: X% of tests executed this sprint
- **Pass Rate**: Y% of executed tests passed
- **Coverage**: Z% of requirements have tests
- **Defects Found**: N defects found via testing
- **Automation**: M% of tests automated

### Test Run Dashboard
- **Progress**: 45/100 tests executed
- **Results**: 40 passed, 3 failed, 2 blocked
- **Duration**: 2h 15m total execution time
- **Defects**: 5 defects logged
- **Assignees**: 3 testers active

### Trend Charts
- Pass rate over last 10 runs (line chart)
- Execution velocity (tests per day)
- Defect discovery rate
- Coverage growth over time

---

## ğŸ”§ Technical Implementation Details

### New API Endpoints (20+)

**Projects**:
- `GET /api/projects` - List projects
- `POST /api/projects` - Create project
- `GET /api/projects/[id]` - Get project
- `PUT /api/projects/[id]` - Update project
- `DELETE /api/projects/[id]` - Delete project

**Test Cases**:
- `GET /api/projects/[id]/tests` - List tests (with filters)
- `POST /api/projects/[id]/tests` - Create test
- `GET /api/projects/[id]/tests/[testId]` - Get test
- `PUT /api/projects/[id]/tests/[testId]` - Update test
- `DELETE /api/projects/[id]/tests/[testId]` - Delete test
- `POST /api/projects/[id]/tests/bulk-import` - Import from AI

**Test Runs**:
- `GET /api/projects/[id]/runs` - List runs
- `POST /api/projects/[id]/runs` - Create run
- `GET /api/projects/[id]/runs/[runId]` - Get run
- `PUT /api/projects/[id]/runs/[runId]` - Update run

**Test Results**:
- `GET /api/projects/[id]/runs/[runId]/results` - Get results
- `POST /api/projects/[id]/runs/[runId]/results` - Submit result
- `PUT /api/projects/[id]/runs/[runId]/results/[resultId]` - Update result

**Requirements**:
- `GET /api/projects/[id]/requirements` - List requirements
- `POST /api/projects/[id]/requirements` - Create requirement
- `POST /api/projects/[id]/requirements/import` - Import from Jira/GitHub

**Coverage**:
- `GET /api/projects/[id]/coverage` - Get coverage matrix
- `POST /api/projects/[id]/coverage` - Link test to requirement
- `DELETE /api/projects/[id]/coverage/[id]` - Unlink

**Reports**:
- `GET /api/projects/[id]/reports/execution` - Execution report
- `GET /api/projects/[id]/reports/coverage` - Coverage report
- `GET /api/projects/[id]/reports/quality` - Quality metrics

---

## ğŸ¯ Success Criteria

### After Phase 1 (Test Management)
- âœ… Can create projects
- âœ… Can create/edit/delete test cases
- âœ… Can organize tests in folders
- âœ… Can tag and search tests
- âœ… Can import AI-generated tests into library

### After Phase 2 (Test Execution)
- âœ… Can create test runs
- âœ… Can execute tests with pass/fail
- âœ… Can attach evidence (screenshots)
- âœ… Can view execution reports

### After Phase 3 (Traceability)
- âœ… Can create/import requirements
- âœ… Can link tests to requirements
- âœ… Can view coverage matrix
- âœ… Can identify untested requirements

### After Phase 4 (Advanced Features)
- âœ… Test review workflow functional
- âœ… Comments and notifications working
- âœ… Dashboards with charts
- âœ… Jira/GitHub integration working

### After Phase 5 (AI Enhancements)
- âœ… AI generates tests from requirements
- âœ… AI suggests missing test scenarios
- âœ… AI detects outdated tests
- âœ… Natural language test creation

---

## ğŸ“… Estimated Timeline

### Sprint 1 (Week 1-2): Foundation
- **Phase 1**: Test Management Foundation (8-10 hours)
- **Outcome**: Can manage test cases in projects

### Sprint 2 (Week 3-4): Execution
- **Phase 2**: Test Execution & Tracking (6-8 hours)
- **Outcome**: Can run tests and track results

### Sprint 3 (Week 5-6): Traceability
- **Phase 3**: Requirements & Traceability (4-6 hours)
- **Outcome**: Coverage tracking functional

### Sprint 4 (Week 7-8): Polish
- **Phase 4**: Advanced QA Features (8-10 hours)
- **Outcome**: Enterprise-ready with integrations

### Sprint 5 (Week 9-10): AI
- **Phase 5**: AI Enhancements (6-8 hours)
- **Outcome**: AI-powered test intelligence

**Total Estimated Time**: 32-42 hours (8-10 weeks at 4h/week)

---

## ğŸ”— Reference Tools to Study

### TestRail (Commercial)
- **URL**: https://www.testrail.com/
- **What to Study**:
  - Test case organization (sections/suites)
  - Test run creation workflow
  - Execution interface (pass/fail per step)
  - Results dashboard
  - Reporting formats

### Xray for Jira (Commercial)
- **URL**: https://www.getxray.app/
- **What to Study**:
  - Requirements-to-tests traceability
  - Test execution screen
  - Coverage reports
  - Jira integration patterns

### Zephyr Scale (Commercial)
- **URL**: https://www.getzephyr.com/
- **What to Study**:
  - Folder structure for tests
  - Execution cycles
  - Test versioning

### qTest (Commercial)
- **URL**: https://www.tricentis.com/products/unified-test-management-qtest
- **What to Study**:
  - Defect linking
  - Test data management
  - Release planning

### Open Source Alternatives to Study
- **TestLink**: https://testlink.org/ (open source, PHP)
- **Kiwi TCMS**: https://kiwitcms.org/ (open source, Python/Django)
- **Allure**: https://docs.qameta.io/allure/ (reporting framework)

---

## ğŸ“ Key Concepts from Professional QA Tools

### 1. Test Case Structure
- **Title**: Short, descriptive
- **Preconditions**: Setup required before test
- **Steps**: Numbered, actionable steps
- **Expected Results**: Per-step or overall
- **Test Data**: Input values needed
- **Priority**: Critical/High/Medium/Low
- **Type**: Functional, Regression, Smoke, etc.

### 2. Test Execution Workflow
1. **Plan**: Create test run, select tests
2. **Assign**: Assign tests to testers
3. **Execute**: Testers run tests, mark results
4. **Report**: Summary reports generated
5. **Review**: Analyze failures, log defects

### 3. Traceability Matrix
```
Requirement â†’ Test Cases â†’ Test Results â†’ Defects
```
Answer questions like:
- Which tests cover requirement REQ-123?
- If REQ-123 changes, which tests need updating?
- What's the pass rate for REQ-123's tests?

### 4. Quality Metrics
- **Test Coverage**: % of requirements with tests
- **Execution Coverage**: % of tests executed
- **Pass Rate**: % of executed tests that passed
- **Defect Density**: Defects per 1000 lines of code
- **Defect Escape Rate**: Defects found in production

---

## ğŸš€ Quick Start: Phase 1 Implementation

To get started immediately on Phase 1, here's the first task:

### Task 1.1: Create Projects Table & API (2 hours)

**Step 1: Database** (15 min)
```sql
-- Add to lib/db.js initDB()
CREATE TABLE IF NOT EXISTS projects (
  id SERIAL PRIMARY KEY,
  name VARCHAR(200) NOT NULL,
  description TEXT,
  prefix VARCHAR(10) DEFAULT 'TC',
  owner_id INTEGER REFERENCES users(id),
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  is_active BOOLEAN DEFAULT TRUE
);
```

**Step 2: API Routes** (45 min)
Create `app/api/projects/route.js`:
```javascript
import { auth } from '@/auth';
import { query } from '@/lib/db';

export async function GET(request) {
  const session = await auth();
  if (!session?.user?.id) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }

  const result = await query(`
    SELECT * FROM projects
    WHERE owner_id = $1 AND is_active = true
    ORDER BY created_at DESC
  `, [session.user.id]);

  return NextResponse.json({ projects: result.rows });
}

export async function POST(request) {
  const session = await auth();
  if (!session?.user?.id) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }

  const { name, description, prefix } = await request.json();

  const result = await query(`
    INSERT INTO projects (name, description, prefix, owner_id)
    VALUES ($1, $2, $3, $4)
    RETURNING *
  `, [name, description, prefix || 'TC', session.user.id]);

  return NextResponse.json({ project: result.rows[0] });
}
```

**Step 3: Projects Page** (1 hour)
Create `app/projects/page.js` with list of projects and "Create Project" button.

---

## ğŸ’¡ Recommendations

### Priority Order
1. **Phase 1 first** - Without test management, you don't have a QA tool
2. **Phase 2 second** - Execution is the core workflow
3. **Phase 3 third** - Traceability adds professional value
4. **Phase 4 & 5** - Nice-to-have enhancements

### MVP Scope (Fastest Path to Value)
If you want to ship quickly, focus on:
- Phase 1: Projects + Test Cases (8 hours)
- Phase 2: Test Runs + Execution (6 hours)
- **Total**: 14 hours for a functional QA tool

Skip initially:
- Requirements/Coverage (can add later)
- Advanced reporting (simple reports first)
- Integrations (manual entry works)
- AI enhancements (existing AI gen is fine)

---

## ğŸ“š Next Steps

1. **Review this plan** - Decide which phases to implement
2. **Study reference tools** - Sign up for TestRail/Xray free trials
3. **Create Phase 1 tasks** - Break down into 2-hour work chunks
4. **Start with projects** - Foundation for everything else

Would you like me to:
1. **Start implementing Phase 1** right now?
2. **Create detailed tasks** for a specific phase?
3. **Build a prototype** of the test execution interface?
4. **Set up database schema** for all phases?

---

**This plan transforms ORIZON from a simple artifact generator into a professional QA management platform comparable to TestRail or Xray.** ğŸš€
